{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Datasets\n",
    "\n",
    "Created by Mitas Ray on 2024-11-19.\n",
    "\n",
    "This notebook is used to compare two datasets. The procedure is to \n",
    "1. restrict the datasets to the same datetime window\n",
    "2. perform high-level analysis on the values in the dataset\n",
    "3. train a model with these datasets and see similar accuracy results\n",
    "\n",
    "To run the notebook,\n",
    "- on linux: use `ficc_python/requirements_py310.txt`, and use `>>> pip install jupyter`\n",
    "- on mac: use `ficc_python/requirements_py310_mac_jupyter.txt`\n",
    "\n",
    "Change the following files to enable credentials and the correct working directory:\n",
    "- `automated_training_auxiliary_functions.py::get_creds(...)`\n",
    "- `ficc/app_engine/demo/server/modules/get_creds.py::get_creds(...)`\n",
    "- `automated_training_auxiliary_variables.py::WORKING_DIRECTORY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the autoreload extension\n",
    "%load_ext autoreload\n",
    "# automatically reloads all imported modules when their source code changes\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 5 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "Initialized pandarallel with 5 cores\n",
      "INFO: Pandarallel will run on 5 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "Initialized pandarallel with 5 cores\n",
      "INFO: Pandarallel will run on 5 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "Initialized pandarallel with 5 cores\n",
      "In PRODUCTION mode (to change to TESTING mode, set `TESTING` to `True`); all files and models will be saved and NUM_EPOCHS=100\n",
      "INFO: Pandarallel will run on 5 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "Initialized pandarallel with 5 cores\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# importing from parent directory: https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "\n",
    "from ficc.utils.auxiliary_functions import get_ys_trade_history_features\n",
    "from ficc.utils.gcp_storage_functions import download_data\n",
    "\n",
    "from automated_training_auxiliary_variables import CATEGORICAL_FEATURES, BINARY, NON_CAT_FEATURES, NUM_TRADES_IN_HISTORY_YIELD_SPREAD_MODEL, BUCKET_NAME, MODEL_TO_CUMULATIVE_DATA_PICKLE_FILENAME, WORKING_DIRECTORY\n",
    "from automated_training_auxiliary_functions import STORAGE_CLIENT, MODEL_NAME_TO_KERAS_MODEL, check_that_model_is_supported, fit_encoders, create_input, train_and_evaluate_model, create_summary_of_results, get_optional_arguments_for_process_data, save_model\n",
    "from set_random_seed import set_seed\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/mitas/ficc/ficc_python. If this is incorrect, change it in `automated_training_auxiliary_variables.py::WORKING_DIRECTORY\n"
     ]
    }
   ],
   "source": [
    "print(f'Working directory: {WORKING_DIRECTORY}. If this is incorrect, change it in `automated_training_auxiliary_variables.py::WORKING_DIRECTORY`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'yield_spread_with_similar_trades'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict the data between a start and end datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_datetime(datetime_as_string: datetime | str) -> datetime:\n",
    "    if isinstance(datetime_as_string, datetime): return datetime_as_string\n",
    "    string_format = '%Y-%m-%d %H:%M:%S'\n",
    "    try:\n",
    "        return datetime.strptime(datetime_as_string, string_format)\n",
    "    except Exception as e:\n",
    "        print(f'{datetime_as_string} must be in {string_format} format')\n",
    "        raise e\n",
    "\n",
    "\n",
    "def restrict_data_to_specified_time_window(data: pd.DataFrame, \n",
    "                                           datetime_column_name: str, \n",
    "                                           start_datetime: datetime | str, \n",
    "                                           end_datetime: datetime | str) -> pd.DataFrame:\n",
    "    '''Return a truncated version of `data` with values of `datetime_column_name` between \n",
    "    `start_datetime` and `end_datetime`.'''\n",
    "    start_datetime, end_datetime = string_to_datetime(start_datetime), string_to_datetime(end_datetime)\n",
    "    after_start_datetime = data[datetime_column_name] >= start_datetime\n",
    "    before_end_datetime = data[datetime_column_name] <= end_datetime\n",
    "    rows_to_keep = after_start_datetime & before_end_datetime\n",
    "    rows_remaining = rows_to_keep.sum()\n",
    "    print(f'{len(data) - rows_remaining} rows removed from the original {len(data)} rows. {rows_remaining} rows remain.')\n",
    "    return data[rows_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_1_start_of_day = '2024-05-01 00:00:00'\n",
    "october_1_start_of_day = '2024-10-01 00:00:00'\n",
    "october_31_end_of_day = '2024-10-31 23:59:59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_data_on_trade_datetime(data: pd.DataFrame, start_datetime_as_string: str, end_datetime_as_string: str) -> pd.DataFrame:\n",
    "    return restrict_data_to_specified_time_window(data, 'trade_datetime', start_datetime_as_string, end_datetime_as_string)\n",
    "\n",
    "\n",
    "def restrict_data_to_october_on_trade_datetime(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    return restrict_data_on_trade_datetime(data, october_1_start_of_day, october_31_end_of_day)\n",
    "\n",
    "\n",
    "def restrict_data_from_may_to_october_on_trade_datetime(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    return restrict_data_on_trade_datetime(data, may_1_start_of_day, october_31_end_of_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data from Google Cloud Storage (or locally saved files) and restrict to desired dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_DIRECTORY = f'{WORKING_DIRECTORY}/files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_file_path = f'{FILES_DIRECTORY}/old_data.pkl'\n",
    "if os.path.isfile(old_data_file_path):\n",
    "    old_data = pd.read_pickle(old_data_file_path)\n",
    "else:\n",
    "    old_data = download_data(STORAGE_CLIENT, BUCKET_NAME, MODEL_TO_CUMULATIVE_DATA_PICKLE_FILENAME[MODEL])\n",
    "    old_data.to_pickle(old_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows removed from the original 6419831 rows. 6419831 rows remain.\n"
     ]
    }
   ],
   "source": [
    "old_data = restrict_data_from_may_to_october_on_trade_datetime(old_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_from_sp_data_for_modeling_bucket(file_names: list) -> pd.DataFrame:\n",
    "    '''Download each file in `file_names` and concatenate the dataframes together.'''\n",
    "    return pd.concat([download_data(STORAGE_CLIENT, 'sp_data_for_modeling', file_name) for file_name in file_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_file_path = f'{FILES_DIRECTORY}/new_data.pkl'\n",
    "if os.path.isfile(new_data_file_path):\n",
    "    new_data = pd.read_pickle(new_data_file_path)\n",
    "else:\n",
    "    new_data = download_files_from_sp_data_for_modeling_bucket(['trades_2024-05-01_to_2024-05-31.pkl', \n",
    "                                                                'trades_2024-06-01_to_2024-06-30.pkl', \n",
    "                                                                'trades_2024-07-01_to_2024-07-31.pkl', \n",
    "                                                                'trades_2024-08-01_to_2024-08-31.pkl', \n",
    "                                                                'trades_2024-09-01_to_2024-09-30.pkl', \n",
    "                                                                'trades_2024-10-01_to_2024-10-31.pkl'])\n",
    "    new_data.to_pickle(new_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows removed from the original 6463891 rows. 6463891 rows remain.\n"
     ]
    }
   ],
   "source": [
    "new_data = restrict_data_from_may_to_october_on_trade_datetime(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_shapes(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Dataset Shapes ===')\n",
    "    print(f'Dataset 1 Shape: {df1.shape}')\n",
    "    print(f'Dataset 2 Shape: {df2.shape}')\n",
    "    num_rows_df1, num_rows_df2 = df1.shape[0], df2.shape[0]\n",
    "    num_rows_difference = num_rows_df1 - num_rows_df2\n",
    "    if num_rows_difference == 0:\n",
    "        print('Both datasets have the same number of rows')\n",
    "    elif num_rows_difference > 0:\n",
    "        print(f'Dataset 1 has {num_rows_difference} ({round(num_rows_difference / num_rows_df2 * 100, 3)}%) more rows than Dataset 2')\n",
    "    elif num_rows_difference < 0:\n",
    "        print(f'Dataset 2 has {abs(num_rows_difference)} ({round(abs(num_rows_difference) / num_rows_df1 * 100, 3)}%) more rows than Dataset 1')\n",
    "    else:\n",
    "        raise ValueError(f'{num_rows_difference} has a value that cannot be compared to 0')\n",
    "\n",
    "\n",
    "def compare_columns(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Column Comparison ===')\n",
    "    print(f'Dataset 1 Columns: {df1.columns.tolist()}')\n",
    "    print(f'Dataset 2 Columns: {df2.columns.tolist()}')\n",
    "\n",
    "    print('\\n=== Common and Unique Columns ===')\n",
    "    common_cols = set(df1.columns).intersection(set(df2.columns))\n",
    "    unique_to_df1 = set(df1.columns) - set(df2.columns)\n",
    "    unique_to_df2 = set(df2.columns) - set(df1.columns)\n",
    "    print(f'Common Columns: {common_cols}')\n",
    "    print(f'Columns only in Dataset 1: {unique_to_df1}')\n",
    "    print(f'Columns only in Dataset 2: {unique_to_df2}')\n",
    "\n",
    "\n",
    "def compare_data_types(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Data Types ===')\n",
    "    print('Dataset 1 Data Types:')\n",
    "    print(df1.dtypes)\n",
    "    print('\\nDataset 2 Data Types:')\n",
    "    print(df2.dtypes)\n",
    "\n",
    "    ## below code does not work if there is a column with dtype numpy array\n",
    "    # print('\\n=== Unique Values per Column ===')\n",
    "    # print('Dataset 1 Unique Values:')\n",
    "    # print(df1.nunique())\n",
    "    # print('\\nDataset 2 Unique Values:')\n",
    "    # print(df2.nunique())\n",
    "\n",
    "\n",
    "def missing_values(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Missing Values ===')\n",
    "    print('Dataset 1 Missing Values:')\n",
    "    missing_df1 = df1.isnull().sum()\n",
    "    print(missing_df1[missing_df1 > 0])\n",
    "    \n",
    "    print('\\nDataset 2 Missing Values:')\n",
    "    missing_df2 = df2.isnull().sum()\n",
    "    print(missing_df2[missing_df2 > 0])\n",
    "\n",
    "\n",
    "def check_last_trade_in_history(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Last Trade in History ===')\n",
    "    columns_to_check = ['last_rtrs_control_number']\n",
    "    column_to_merge_on = 'rtrs_control_number'\n",
    "    columns_to_keep = [column_to_merge_on] + columns_to_check\n",
    "    assert all([((column in df1.columns) and (column in df2.columns)) for column in columns_to_keep]), f'Not all columns in {columns_to_keep} are present in both datasets'\n",
    "    df1, df2 = df1[columns_to_keep], df2[columns_to_keep]\n",
    "    suffix1, suffix2 = '_df1', '_df2'\n",
    "    merged_df = pd.merge(df1, df2, on=column_to_merge_on, suffixes=(suffix1, suffix2))\n",
    "    for column in columns_to_check:\n",
    "        print(f'{column}: {(merged_df[column + suffix1] != merged_df[column + suffix2]).sum()} rows have different values for the same {column_to_merge_on}')\n",
    "\n",
    "\n",
    "def statistical_summary(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    '''`.describe(...)` has issues if there is a column with dtype numpy array.'''\n",
    "    print('\\n=== Statistical Summary ===')\n",
    "    print('Dataset 1 Summary:')\n",
    "    print(df1.describe(include='all'))\n",
    "    print('\\nDataset 2 Summary:')\n",
    "    print(df2.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Shapes ===\n",
      "Dataset 1 Shape: (6419831, 141)\n",
      "Dataset 2 Shape: (6463891, 139)\n",
      "Dataset 2 has 44060 (0.686%) more rows than Dataset 1\n",
      "\n",
      "=== Column Comparison ===\n",
      "Dataset 1 Columns: ['rtrs_control_number', 'cusip', 'yield', 'is_callable', 'refund_date', 'accrual_date', 'dated_date', 'next_sink_date', 'coupon', 'delivery_date', 'trade_date', 'trade_datetime', 'par_call_date', 'interest_payment_frequency', 'is_called', 'is_non_transaction_based_compensation', 'is_general_obligation', 'callable_at_cav', 'extraordinary_make_whole_call', 'make_whole_call', 'has_unexpired_lines_of_credit', 'escrow_exists', 'incorporated_state_code', 'trade_type', 'par_traded', 'maturity_date', 'settlement_date', 'next_call_date', 'issue_amount', 'maturity_amount', 'issue_price', 'orig_principal_amount', 'max_amount_outstanding', 'dollar_price', 'calc_date', 'purpose_sub_class', 'called_redemption_type', 'calc_day_cat', 'previous_coupon_payment_date', 'instrument_primary_name', 'purpose_class', 'call_timing', 'call_timing_in_part', 'sink_frequency', 'sink_amount_type', 'issue_text', 'state_tax_status', 'series_name', 'transaction_type', 'next_call_price', 'par_call_price', 'when_issued', 'min_amount_outstanding', 'original_yield', 'par_price', 'default_indicator', 'sp_stand_alone', 'sp_long', 'moodys_long', 'coupon_type', 'federal_tax_status', 'use_of_proceeds', 'muni_security_type', 'muni_issue_type', 'capital_type', 'other_enhancement_type', 'next_coupon_payment_date', 'first_coupon_date', 'last_period_accrues_from_date', 'rating', 'trade_history', 'last_yield_spread', 'last_ficc_ycl', 'last_rtrs_control_number', 'last_yield', 'last_dollar_price', 'last_seconds_ago', 'last_size', 'last_calc_date', 'last_maturity_date', 'last_next_call_date', 'last_par_call_date', 'last_refund_date', 'last_trade_datetime', 'last_calc_day_cat', 'last_settlement_date', 'last_trade_type', 'similar_trade_history', 'ficc_ycl', 'yield_spread', 'treasury_rate', 'ficc_treasury_spread', 'quantity', 'callable', 'called', 'zerocoupon', 'whenissued', 'sinking', 'deferred', 'days_to_settle', 'days_to_maturity', 'days_to_call', 'days_to_refund', 'days_to_par', 'call_to_maturity', 'accrued_days', 'days_in_interest_payment', 'scaled_accrued_days', 'A/E', 'last_trade_date', 'new_ficc_ycl', 'target_attention_features', 'new_ys', 'max_ys_ys', 'max_ys_ttypes', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ttypes', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ttypes', 'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ttypes', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ttypes', 'D_min_ago_ago', 'D_min_ago_qdiff', 'P_min_ago_ys', 'P_min_ago_ttypes', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ttypes', 'S_min_ago_ago', 'S_min_ago_qdiff']\n",
      "Dataset 2 Columns: ['rtrs_control_number', 'cusip', 'yield', 'is_callable', 'refund_date', 'accrual_date', 'dated_date', 'next_sink_date', 'coupon', 'delivery_date', 'trade_date', 'trade_datetime', 'par_call_date', 'interest_payment_frequency', 'is_called', 'is_non_transaction_based_compensation', 'is_general_obligation', 'callable_at_cav', 'extraordinary_make_whole_call', 'make_whole_call', 'has_unexpired_lines_of_credit', 'escrow_exists', 'incorporated_state_code', 'trade_type', 'par_traded', 'maturity_date', 'settlement_date', 'next_call_date', 'issue_amount', 'maturity_amount', 'issue_price', 'orig_principal_amount', 'max_amount_outstanding', 'dollar_price', 'calc_date', 'purpose_sub_class', 'called_redemption_type', 'calc_day_cat', 'previous_coupon_payment_date', 'instrument_primary_name', 'purpose_class', 'call_timing', 'call_timing_in_part', 'sink_frequency', 'sink_amount_type', 'issue_text', 'state_tax_status', 'series_name', 'transaction_type', 'next_call_price', 'par_call_price', 'when_issued', 'min_amount_outstanding', 'original_yield', 'par_price', 'default_indicator', 'sp_long', 'coupon_type', 'federal_tax_status', 'use_of_proceeds', 'muni_security_type', 'muni_issue_type', 'capital_type', 'other_enhancement_type', 'next_coupon_payment_date', 'first_coupon_date', 'last_period_accrues_from_date', 'rating', 'trade_history', 'last_yield_spread', 'last_ficc_ycl', 'last_rtrs_control_number', 'last_yield', 'last_dollar_price', 'last_seconds_ago', 'last_size', 'last_calc_date', 'last_maturity_date', 'last_next_call_date', 'last_par_call_date', 'last_refund_date', 'last_trade_datetime', 'last_calc_day_cat', 'last_settlement_date', 'last_trade_type', 'similar_trade_history', 'ficc_ycl', 'yield_spread', 'treasury_rate', 'ficc_treasury_spread', 'quantity', 'callable', 'called', 'zerocoupon', 'whenissued', 'sinking', 'deferred', 'days_to_settle', 'days_to_maturity', 'days_to_call', 'days_to_refund', 'days_to_par', 'call_to_maturity', 'accrued_days', 'days_in_interest_payment', 'scaled_accrued_days', 'A/E', 'last_trade_date', 'new_ficc_ycl', 'target_attention_features', 'new_ys', 'max_ys_ys', 'max_ys_ttypes', 'max_ys_ago', 'max_ys_qdiff', 'min_ys_ys', 'min_ys_ttypes', 'min_ys_ago', 'min_ys_qdiff', 'max_qty_ys', 'max_qty_ttypes', 'max_qty_ago', 'max_qty_qdiff', 'min_ago_ys', 'min_ago_ttypes', 'min_ago_ago', 'min_ago_qdiff', 'D_min_ago_ys', 'D_min_ago_ttypes', 'D_min_ago_ago', 'D_min_ago_qdiff', 'P_min_ago_ys', 'P_min_ago_ttypes', 'P_min_ago_ago', 'P_min_ago_qdiff', 'S_min_ago_ys', 'S_min_ago_ttypes', 'S_min_ago_ago', 'S_min_ago_qdiff']\n",
      "\n",
      "=== Common and Unique Columns ===\n",
      "Common Columns: {'S_min_ago_ys', 'muni_issue_type', 'purpose_class', 'last_par_call_date', 'previous_coupon_payment_date', 'last_settlement_date', 'min_ago_qdiff', 'call_timing', 'last_trade_type', 'rtrs_control_number', 'original_yield', 'state_tax_status', 'sinking', 'last_yield', 'dollar_price', 'last_dollar_price', 'cusip', 'issue_amount', 'par_traded', 'deferred', 'min_ago_ago', 'called', 'capital_type', 'last_size', 'call_to_maturity', 'purpose_sub_class', 'S_min_ago_ago', 'max_qty_ys', 'target_attention_features', 'days_to_settle', 'trade_type', 'interest_payment_frequency', 'last_seconds_ago', 'days_to_refund', 'yield', 'dated_date', 'refund_date', 'min_amount_outstanding', 'min_ago_ys', 'last_ficc_ycl', 'min_ys_qdiff', 'coupon_type', 'P_min_ago_ttypes', 'new_ys', 'last_calc_date', 'is_non_transaction_based_compensation', 'is_callable', 'P_min_ago_ys', 'muni_security_type', 'rating', 'scaled_accrued_days', 'settlement_date', 'issue_price', 'is_general_obligation', 'last_refund_date', 'new_ficc_ycl', 'next_coupon_payment_date', 'next_call_price', 'max_ys_ttypes', 'sink_frequency', 'days_to_maturity', 'callable_at_cav', 'treasury_rate', 'trade_date', 'when_issued', 'ficc_ycl', 'D_min_ago_ago', 'calc_date', 'max_ys_ago', 'max_amount_outstanding', 'calc_day_cat', 'call_timing_in_part', 'transaction_type', 'quantity', 'P_min_ago_qdiff', 'escrow_exists', 'ficc_treasury_spread', 'extraordinary_make_whole_call', 'max_ys_ys', 'sink_amount_type', 'issue_text', 'min_ago_ttypes', 'next_sink_date', 'first_coupon_date', 'max_ys_qdiff', 'min_ys_ys', 'S_min_ago_qdiff', 'par_call_date', 'P_min_ago_ago', 'D_min_ago_ys', 'next_call_date', 'trade_history', 'accrued_days', 'min_ys_ttypes', 'default_indicator', 'par_call_price', 'par_price', 'series_name', 'last_next_call_date', 'accrual_date', 'has_unexpired_lines_of_credit', 'D_min_ago_qdiff', 'is_called', 'delivery_date', 'federal_tax_status', 'yield_spread', 'A/E', 'maturity_date', 'orig_principal_amount', 'instrument_primary_name', 'days_to_par', 'zerocoupon', 'D_min_ago_ttypes', 'min_ys_ago', 'whenissued', 'last_trade_date', 'callable', 'trade_datetime', 'incorporated_state_code', 'sp_long', 'days_in_interest_payment', 'maturity_amount', 'use_of_proceeds', 'last_rtrs_control_number', 'last_calc_day_cat', 'other_enhancement_type', 'similar_trade_history', 'last_yield_spread', 'coupon', 'last_maturity_date', 'last_trade_datetime', 'S_min_ago_ttypes', 'called_redemption_type', 'max_qty_ago', 'max_qty_qdiff', 'last_period_accrues_from_date', 'days_to_call', 'make_whole_call', 'max_qty_ttypes'}\n",
      "Columns only in Dataset 1: {'moodys_long', 'sp_stand_alone'}\n",
      "Columns only in Dataset 2: set()\n",
      "\n",
      "=== Missing Values ===\n",
      "Dataset 1 Missing Values:\n",
      "refund_date                      6178845\n",
      "next_sink_date                   5071097\n",
      "par_call_date                    2101210\n",
      "next_call_date                   2097316\n",
      "purpose_sub_class                1360240\n",
      "previous_coupon_payment_date      951110\n",
      "instrument_primary_name              284\n",
      "original_yield                      7187\n",
      "moodys_long                      1771008\n",
      "use_of_proceeds                       30\n",
      "muni_issue_type                  6152731\n",
      "other_enhancement_type           5524583\n",
      "next_coupon_payment_date              10\n",
      "first_coupon_date                      1\n",
      "last_period_accrues_from_date      87720\n",
      "last_ficc_ycl                     157589\n",
      "last_rtrs_control_number          157589\n",
      "last_yield                        157589\n",
      "last_size                         157589\n",
      "last_calc_date                    157589\n",
      "last_maturity_date                157589\n",
      "last_next_call_date              2188720\n",
      "last_par_call_date               2192293\n",
      "last_refund_date                 6187579\n",
      "last_trade_datetime               157589\n",
      "last_calc_day_cat                 157589\n",
      "last_settlement_date              157589\n",
      "last_trade_type                   157589\n",
      "last_trade_date                   157589\n",
      "dtype: int64\n",
      "\n",
      "Dataset 2 Missing Values:\n",
      "refund_date                      6244078\n",
      "next_sink_date                   5120737\n",
      "delivery_date                         75\n",
      "par_call_date                    2106888\n",
      "next_call_date                   2098456\n",
      "purpose_sub_class                 737874\n",
      "previous_coupon_payment_date      842108\n",
      "instrument_primary_name              126\n",
      "original_yield                     12916\n",
      "use_of_proceeds                        8\n",
      "muni_issue_type                  6324354\n",
      "other_enhancement_type           5942843\n",
      "next_coupon_payment_date          319947\n",
      "first_coupon_date                     31\n",
      "last_period_accrues_from_date      97437\n",
      "last_ficc_ycl                     178028\n",
      "last_rtrs_control_number          178028\n",
      "last_yield                        178028\n",
      "last_size                         178028\n",
      "last_calc_date                    178028\n",
      "last_maturity_date                178028\n",
      "last_next_call_date              2216371\n",
      "last_par_call_date               2220026\n",
      "last_refund_date                 6231574\n",
      "last_trade_datetime               178028\n",
      "last_calc_day_cat                 178028\n",
      "last_settlement_date              178028\n",
      "last_trade_type                   178028\n",
      "last_trade_date                   178028\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "compare_shapes(old_data, new_data)\n",
    "compare_columns(old_data, new_data)\n",
    "# compare_data_types(old_data, new_data)\n",
    "missing_values(old_data, new_data)\n",
    "check_last_trade_in_history(old_data, new_data)\n",
    "# statistical_summary(old_data, new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train yield spread with similar trades model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_for_each_trade_in_history() -> int:\n",
    "    optional_arguments_for_process_data = get_optional_arguments_for_process_data(MODEL)\n",
    "    use_treasury_spread = optional_arguments_for_process_data.get('use_treasury_spread', False)\n",
    "    trade_history_features = get_ys_trade_history_features(use_treasury_spread)\n",
    "    return len(trade_history_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data: pd.DataFrame, \n",
    "                last_trade_date_for_training_dataset: str):\n",
    "    '''Heavily inspired by `automated_trianing_auxiliary_functions::train_model(...)`. The main changes are: \n",
    "    (1) assume that we are using the yield spread with similar trades model,\n",
    "    (2) do not have an exclusions function\n",
    "    (3) do not restrict the test set to just a single day\n",
    "    '''\n",
    "    check_that_model_is_supported(MODEL)\n",
    "    encoders, fmax = fit_encoders(data, CATEGORICAL_FEATURES, MODEL)\n",
    "    test_data = data[data.trade_date > last_trade_date_for_training_dataset]    # `test_data` can only contain trades after `last_trade_date_for_training_dataset`\n",
    "    train_data = data[data.trade_date <= last_trade_date_for_training_dataset]    # `train_data` only contains trades before and including `last_trade_date_for_training_dataset`\n",
    "    training_set_info = f'Training set contains {len(train_data)} trades ranging from trade datetimes of {train_data.trade_datetime.min()} to {train_data.trade_datetime.max()}'\n",
    "    test_set_info = f'Test set contains {len(test_data)} trades ranging from trade datetimes of {test_data.trade_datetime.min()} to {test_data.trade_datetime.max()}'\n",
    "    print(training_set_info)\n",
    "    print(test_set_info)\n",
    "\n",
    "    x_train, y_train = create_input(train_data, encoders, MODEL)\n",
    "    x_test, y_test = create_input(test_data, encoders, MODEL)\n",
    "\n",
    "    keras_model = MODEL_NAME_TO_KERAS_MODEL[MODEL]\n",
    "    untrained_model = keras_model(x_train, \n",
    "                                  NUM_TRADES_IN_HISTORY_YIELD_SPREAD_MODEL, \n",
    "                                  get_num_features_for_each_trade_in_history(), \n",
    "                                  CATEGORICAL_FEATURES, \n",
    "                                  NON_CAT_FEATURES, \n",
    "                                  BINARY, \n",
    "                                  fmax)\n",
    "    trained_model, mae, history = train_and_evaluate_model(untrained_model, x_train, y_train, x_test, y_test)\n",
    "    result_df = create_summary_of_results(trained_model, test_data, x_test, y_test)\n",
    "    save_model(trained_model, None, MODEL, 'old_data', upload_to_google_cloud_bucket=False)    # setting `encoders=None` to not save the encoders file\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set contains 5257795 trades ranging from trade datetimes of 2024-05-01 07:01:05 to 2024-09-30 18:24:32\n",
      "Test set contains 1162036 trades ranging from trade datetimes of 2024-10-01 00:00:00 to 2024-10-31 18:35:10\n",
      "BEGIN create_input\n",
      "END create_input. Execution time: 0:00:32.438\n",
      "BEGIN create_input\n",
      "END create_input. Execution time: 0:00:07.218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 15:13:43.898232: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4732/4733 [============================>.] - ETA: 0s - loss: 33.2542 - mean_absolute_error: 33.2542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 15:18:21.142926: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:693] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4733/4733 [==============================] - 294s 61ms/step - loss: 33.2542 - mean_absolute_error: 33.2542 - val_loss: 15.0574 - val_mean_absolute_error: 15.0574\n",
      "Epoch 2/100\n",
      "4733/4733 [==============================] - 284s 60ms/step - loss: 14.8060 - mean_absolute_error: 14.8060 - val_loss: 13.7381 - val_mean_absolute_error: 13.7381\n",
      "Epoch 3/100\n",
      "4733/4733 [==============================] - 292s 62ms/step - loss: 13.9289 - mean_absolute_error: 13.9289 - val_loss: 14.1743 - val_mean_absolute_error: 14.1743\n",
      "Epoch 4/100\n",
      "4733/4733 [==============================] - 293s 62ms/step - loss: 13.6047 - mean_absolute_error: 13.6047 - val_loss: 13.8678 - val_mean_absolute_error: 13.8678\n",
      "Epoch 5/100\n",
      "4733/4733 [==============================] - 287s 61ms/step - loss: 13.3866 - mean_absolute_error: 13.3866 - val_loss: 13.0431 - val_mean_absolute_error: 13.0431\n",
      "Epoch 6/100\n",
      "3078/4733 [==================>...........] - ETA: 1:38 - loss: 13.2451 - mean_absolute_error: 13.2451"
     ]
    }
   ],
   "source": [
    "train_model(old_data, '2024-09-30')    # Tuesday 2024-10-01 - Thursday 2024-10-31 is the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(new_data, '2024-09-30')    # Tuesday 2024-10-01 - Thursday 2024-10-31 is the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
