{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Datasets\n",
    "\n",
    "Created by Mitas Ray on 2024-11-19.\n",
    "\n",
    "This notebook is used to compare two datasets. The procedure is to \n",
    "1. restrict the datasets to the same datetime window\n",
    "2. perform high-level analysis on the values in the dataset\n",
    "3. train a model with these datasets and see similar accuracy results\n",
    "\n",
    "To run the notebook,\n",
    "- on linux: use `ficc_python/requirements_py310.txt`, and use `>>> pip install jupyter`\n",
    "- on mac: use `ficc_python/requirements_py310_mac_jupyter.txt`\n",
    "\n",
    "Change the following files to enable credentials and the correct working directory:\n",
    "- `automated_training_auxiliary_functions.py::get_creds(...)`\n",
    "- `ficc/app_engine/demo/server/modules/get_creds.py::get_creds(...)`\n",
    "- `automated_training_auxiliary_variables.py::WORKING_DIRECTORY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the autoreload extension\n",
    "%load_ext autoreload\n",
    "# automatically reloads all imported modules when their source code changes\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# importing from parent directory: https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "\n",
    "from ficc.utils.auxiliary_functions import get_ys_trade_history_features\n",
    "from ficc.utils.gcp_storage_functions import download_data\n",
    "\n",
    "from automated_training_auxiliary_variables import CATEGORICAL_FEATURES, BINARY, NON_CAT_FEATURES, NUM_TRADES_IN_HISTORY_YIELD_SPREAD_MODEL, BUCKET_NAME, MODEL_TO_CUMULATIVE_DATA_PICKLE_FILENAME, WORKING_DIRECTORY\n",
    "from automated_training_auxiliary_functions import STORAGE_CLIENT, MODEL_NAME_TO_KERAS_MODEL, check_that_model_is_supported, fit_encoders, create_input, train_and_evaluate_model, create_summary_of_results, get_optional_arguments_for_process_data, save_model\n",
    "from set_random_seed import set_seed\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Working directory: {WORKING_DIRECTORY}. If this is incorrect, change it in `automated_training_auxiliary_variables.py::WORKING_DIRECTORY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'yield_spread_with_similar_trades'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict the data between a start and end datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_datetime(datetime_as_string: datetime | str) -> datetime:\n",
    "    if isinstance(datetime_as_string, datetime): return datetime_as_string\n",
    "    string_format = '%Y-%m-%d %H:%M:%S'\n",
    "    try:\n",
    "        return datetime.strptime(datetime_as_string, string_format)\n",
    "    except Exception as e:\n",
    "        print(f'{datetime_as_string} must be in {string_format} format')\n",
    "        raise e\n",
    "\n",
    "\n",
    "def restrict_data_to_specified_time_window(data: pd.DataFrame, \n",
    "                                           datetime_column_name: str, \n",
    "                                           start_datetime: datetime | str, \n",
    "                                           end_datetime: datetime | str) -> pd.DataFrame:\n",
    "    '''Return a truncated version of `data` with values of `datetime_column_name` between \n",
    "    `start_datetime` and `end_datetime`.'''\n",
    "    start_datetime, end_datetime = string_to_datetime(start_datetime), string_to_datetime(end_datetime)\n",
    "    after_start_datetime = data[datetime_column_name] >= start_datetime\n",
    "    before_end_datetime = data[datetime_column_name] <= end_datetime\n",
    "    rows_to_keep = after_start_datetime & before_end_datetime\n",
    "    rows_remaining = rows_to_keep.sum()\n",
    "    print(f'{len(data) - rows_remaining} rows removed from the original {len(data)} rows. {rows_remaining} rows remain.')\n",
    "    return data[rows_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_1_start_of_day = '2024-05-01 00:00:00'\n",
    "october_1_start_of_day = '2024-10-01 00:00:00'\n",
    "october_31_end_of_day = '2024-10-31 23:59:59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_data_on_trade_datetime(data: pd.DataFrame, start_datetime_as_string: str, end_datetime_as_string: str) -> pd.DataFrame:\n",
    "    return restrict_data_to_specified_time_window(data, 'trade_datetime', start_datetime_as_string, end_datetime_as_string)\n",
    "\n",
    "\n",
    "def restrict_data_to_october_on_trade_datetime(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    return restrict_data_on_trade_datetime(data, october_1_start_of_day, october_31_end_of_day)\n",
    "\n",
    "\n",
    "def restrict_data_from_may_to_october_on_trade_datetime(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    return restrict_data_on_trade_datetime(data, may_1_start_of_day, october_31_end_of_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data from Google Cloud Storage (or locally saved files) and restrict to desired dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_DIRECTORY = f'{WORKING_DIRECTORY}/files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_file_path = f'{FILES_DIRECTORY}/old_data.pkl'\n",
    "if os.path.isfile(old_data_file_path):\n",
    "    old_data = pd.read_pickle(old_data_file_path)\n",
    "else:\n",
    "    old_data = download_data(STORAGE_CLIENT, BUCKET_NAME, MODEL_TO_CUMULATIVE_DATA_PICKLE_FILENAME[MODEL])\n",
    "    old_data.to_pickle(old_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = restrict_data_from_may_to_october_on_trade_datetime(old_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_from_sp_data_for_modeling_bucket(file_names: list) -> pd.DataFrame:\n",
    "    '''Download each file in `file_names` and concatenate the dataframes together.'''\n",
    "    return pd.concat([download_data(STORAGE_CLIENT, 'sp_data_for_modeling', file_name) for file_name in file_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_file_path = f'{FILES_DIRECTORY}/new_data.pkl'\n",
    "if os.path.isfile(new_data_file_path):\n",
    "    new_data = pd.read_pickle(new_data_file_path)\n",
    "else:\n",
    "    new_data = download_files_from_sp_data_for_modeling_bucket(['trades_2024-05-01_to_2024-05-31.pkl', \n",
    "                                                                'trades_2024-06-01_to_2024-06-30.pkl', \n",
    "                                                                'trades_2024-07-01_to_2024-07-31.pkl', \n",
    "                                                                'trades_2024-08-01_to_2024-08-31.pkl', \n",
    "                                                                'trades_2024-09-01_to_2024-09-30.pkl', \n",
    "                                                                'trades_2024-10-01_to_2024-10-31.pkl'])\n",
    "    new_data.to_pickle(new_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = download_files_from_sp_data_for_modeling_bucket(['trades_2024-05-01_to_2024-05-31.pkl', \n",
    "                                                            'trades_2024-06-01_to_2024-06-30.pkl', \n",
    "                                                            'trades_2024-07-01_to_2024-07-31.pkl', \n",
    "                                                            'trades_2024-08-01_to_2024-08-31.pkl', \n",
    "                                                            'trades_2024-09-01_to_2024-09-30.pkl', \n",
    "                                                            'trades_2024-10-01_to_2024-10-31.pkl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = restrict_data_from_may_to_october_on_trade_datetime(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_shapes(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Dataset Shapes ===')\n",
    "    print(f'Dataset 1 Shape: {df1.shape}')\n",
    "    print(f'Dataset 2 Shape: {df2.shape}')\n",
    "    num_rows_df1, num_rows_df2 = df1.shape[0], df2.shape[0]\n",
    "    num_rows_difference = num_rows_df1 - num_rows_df2\n",
    "    if num_rows_difference == 0:\n",
    "        print('Both datasets have the same number of rows')\n",
    "    elif num_rows_difference > 0:\n",
    "        print(f'Dataset 1 has {num_rows_difference} ({round(num_rows_difference / num_rows_df2 * 100, 3)}%) more rows than Dataset 2')\n",
    "    elif num_rows_difference < 0:\n",
    "        print(f'Dataset 2 has {abs(num_rows_difference)} ({round(abs(num_rows_difference) / num_rows_df1 * 100, 3)}%) more rows than Dataset 1')\n",
    "    else:\n",
    "        raise ValueError(f'{num_rows_difference} has a value that cannot be compared to 0')\n",
    "\n",
    "\n",
    "def compare_columns(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Column Comparison ===')\n",
    "    print(f'Dataset 1 Columns: {df1.columns.tolist()}')\n",
    "    print(f'Dataset 2 Columns: {df2.columns.tolist()}')\n",
    "\n",
    "    print('\\n=== Common and Unique Columns ===')\n",
    "    common_cols = set(df1.columns).intersection(set(df2.columns))\n",
    "    unique_to_df1 = set(df1.columns) - set(df2.columns)\n",
    "    unique_to_df2 = set(df2.columns) - set(df1.columns)\n",
    "    print(f'Common Columns: {common_cols}')\n",
    "    print(f'Columns only in Dataset 1: {unique_to_df1}')\n",
    "    print(f'Columns only in Dataset 2: {unique_to_df2}')\n",
    "\n",
    "\n",
    "def compare_data_types(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Data Types ===')\n",
    "    print('Dataset 1 Data Types:')\n",
    "    print(df1.dtypes)\n",
    "    print('\\nDataset 2 Data Types:')\n",
    "    print(df2.dtypes)\n",
    "\n",
    "    ## below code does not work if there is a column with dtype numpy array\n",
    "    # print('\\n=== Unique Values per Column ===')\n",
    "    # print('Dataset 1 Unique Values:')\n",
    "    # print(df1.nunique())\n",
    "    # print('\\nDataset 2 Unique Values:')\n",
    "    # print(df2.nunique())\n",
    "\n",
    "\n",
    "def missing_values(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Missing Values ===')\n",
    "    print('Dataset 1 Missing Values:')\n",
    "    missing_df1 = df1.isnull().sum()\n",
    "    print(missing_df1[missing_df1 > 0])\n",
    "    \n",
    "    print('\\nDataset 2 Missing Values:')\n",
    "    missing_df2 = df2.isnull().sum()\n",
    "    print(missing_df2[missing_df2 > 0])\n",
    "\n",
    "\n",
    "def statistical_summary(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    '''`.describe(...)` has issues if there is a column with dtype numpy array.'''\n",
    "    print('\\n=== Statistical Summary ===')\n",
    "    print('Dataset 1 Summary:')\n",
    "    print(df1.describe(include='all'))\n",
    "    print('\\nDataset 2 Summary:')\n",
    "    print(df2.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_shapes(old_data, new_data)\n",
    "compare_columns(old_data, new_data)\n",
    "# compare_data_types(old_data, new_data)\n",
    "missing_values(old_data, new_data)\n",
    "# statistical_summary(old_data, new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train yield spread with similar trades model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_for_each_trade_in_history() -> int:\n",
    "    optional_arguments_for_process_data = get_optional_arguments_for_process_data(MODEL)\n",
    "    use_treasury_spread = optional_arguments_for_process_data.get('use_treasury_spread', False)\n",
    "    trade_history_features = get_ys_trade_history_features(use_treasury_spread)\n",
    "    return len(trade_history_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data: pd.DataFrame, \n",
    "                last_trade_date_for_training_dataset: str):\n",
    "    '''Heavily inspired by `automated_trianing_auxiliary_functions::train_model(...)`. The main changes are: \n",
    "    (1) assume that we are using the yield spread with similar trades model,\n",
    "    (2) do not have an exclusions function\n",
    "    (3) do not restrict the test set to just a single day\n",
    "    '''\n",
    "    check_that_model_is_supported(MODEL)\n",
    "    encoders, fmax = fit_encoders(data, CATEGORICAL_FEATURES, MODEL)\n",
    "    test_data = data[data.trade_date > last_trade_date_for_training_dataset]    # `test_data` can only contain trades after `last_trade_date_for_training_dataset`\n",
    "    train_data = data[data.trade_date <= last_trade_date_for_training_dataset]    # `train_data` only contains trades before and including `last_trade_date_for_training_dataset`\n",
    "    training_set_info = f'Training set contains {len(train_data)} trades ranging from trade datetimes of {train_data.trade_datetime.min()} to {train_data.trade_datetime.max()}'\n",
    "    test_set_info = f'Test set contains {len(test_data)} trades ranging from trade datetimes of {test_data.trade_datetime.min()} to {test_data.trade_datetime.max()}'\n",
    "    print(training_set_info)\n",
    "    print(test_set_info)\n",
    "\n",
    "    x_train, y_train = create_input(train_data, encoders, MODEL)\n",
    "    x_test, y_test = create_input(test_data, encoders, MODEL)\n",
    "\n",
    "    keras_model = MODEL_NAME_TO_KERAS_MODEL[MODEL]\n",
    "    untrained_model = keras_model(x_train, \n",
    "                                  NUM_TRADES_IN_HISTORY_YIELD_SPREAD_MODEL, \n",
    "                                  get_num_features_for_each_trade_in_history(), \n",
    "                                  CATEGORICAL_FEATURES, \n",
    "                                  NON_CAT_FEATURES, \n",
    "                                  BINARY, \n",
    "                                  fmax)\n",
    "    trained_model, mae, history = train_and_evaluate_model(untrained_model, x_train, y_train, x_test, y_test)\n",
    "    result_df = create_summary_of_results(trained_model, test_data, x_test, y_test)\n",
    "    save_model(trained_model, None, MODEL, 'old_data', upload_to_google_cloud_bucket=False)    # setting `encoders=None` to not save the encoders file\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(old_data, '2024-09-30')    # Tuesday 2024-10-01 - Thursday 2024-10-31 is the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(new_data, '2024-09-30')    # Tuesday 2024-10-01 - Thursday 2024-10-31 is the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
