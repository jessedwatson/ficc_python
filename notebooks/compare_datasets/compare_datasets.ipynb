{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Datasets\n",
    "\n",
    "Created by Mitas Ray on 2024-11-17.\n",
    "\n",
    "Last edited by Mitas Ray on 2024-11-22.\n",
    "\n",
    "This notebook is used to compare two datasets. The procedure is to \n",
    "1. restrict the datasets to the same datetime window\n",
    "2. perform high-level analysis on the values in the dataset\n",
    "3. train a model with these datasets and see similar accuracy results\n",
    "\n",
    "To run the notebook,\n",
    "- on linux: use `ficc_python/requirements_py310.txt`, and use `>>> pip install jupyter`\n",
    "- on mac: use `ficc_python/requirements_py310_mac_jupyter.txt`\n",
    "- to run the plotting functions at the end of this notebook, use `>>> pip install matplotlib`\n",
    "\n",
    "Change the following files to enable credentials and the correct working directory:\n",
    "- `automated_training_auxiliary_functions.py::get_creds(...)`\n",
    "- `ficc/app_engine/demo/server/modules/get_creds.py::get_creds(...)`\n",
    "- `automated_training_auxiliary_variables.py::WORKING_DIRECTORY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the autoreload extension\n",
    "%load_ext autoreload\n",
    "# automatically reloads all imported modules when their source code changes\n",
    "%autoreload 2\n",
    "# displaying plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# importing from parent directory: https://stackoverflow.com/questions/714063/importing-modules-from-parent-folder\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "\n",
    "from ficc.utils.auxiliary_functions import get_ys_trade_history_features\n",
    "from ficc.utils.gcp_storage_functions import download_data\n",
    "from ficc.pricing.price import compute_price\n",
    "\n",
    "from automated_training_auxiliary_variables import CATEGORICAL_FEATURES, BINARY, NON_CAT_FEATURES, NUM_TRADES_IN_HISTORY_YIELD_SPREAD_MODEL, BATCH_SIZE, BUCKET_NAME, MODEL_TO_CUMULATIVE_DATA_PICKLE_FILENAME, WORKING_DIRECTORY, HOME_DIRECTORY\n",
    "from automated_training_auxiliary_functions import STORAGE_CLIENT, MODEL_NAME_TO_KERAS_MODEL, check_that_model_is_supported, fit_encoders, create_input, train_and_evaluate_model, create_summary_of_results, get_optional_arguments_for_process_data, save_model, apply_exclusions\n",
    "from set_random_seed import set_seed\n",
    "\n",
    "set_seed()\n",
    "\n",
    "\n",
    "sys.path.insert(0, f'{HOME_DIRECTORY}/ficc/ficc/app_engine/demo/server/')\n",
    "\n",
    "\n",
    "from modules.exclusions import missing_important_dates_or_dates_are_out_of_bounds, missing_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Working directory: {WORKING_DIRECTORY}. If this is incorrect, change it in `automated_training_auxiliary_variables.py::WORKING_DIRECTORY`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'yield_spread_with_similar_trades'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict the data between a start and end datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_datetime(datetime_as_string: datetime | str) -> datetime:\n",
    "    if isinstance(datetime_as_string, datetime): return datetime_as_string\n",
    "    string_format = '%Y-%m-%d %H:%M:%S'\n",
    "    try:\n",
    "        return datetime.strptime(datetime_as_string, string_format)\n",
    "    except Exception as e:\n",
    "        print(f'{datetime_as_string} must be in {string_format} format')\n",
    "        raise e\n",
    "\n",
    "\n",
    "def restrict_data_to_specified_time_window(data: pd.DataFrame, \n",
    "                                           datetime_column_name: str, \n",
    "                                           start_datetime: datetime | str, \n",
    "                                           end_datetime: datetime | str) -> pd.DataFrame:\n",
    "    '''Return a truncated version of `data` with values of `datetime_column_name` between \n",
    "    `start_datetime` and `end_datetime`.'''\n",
    "    start_datetime, end_datetime = string_to_datetime(start_datetime), string_to_datetime(end_datetime)\n",
    "    after_start_datetime = data[datetime_column_name] >= start_datetime\n",
    "    before_end_datetime = data[datetime_column_name] <= end_datetime\n",
    "    rows_to_keep = after_start_datetime & before_end_datetime\n",
    "    rows_remaining = rows_to_keep.sum()\n",
    "    print(f'{len(data) - rows_remaining} rows removed from the original {len(data)} rows. {rows_remaining} rows remain.')\n",
    "    return data[rows_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_1_start_of_day = '2024-05-01 00:00:00'\n",
    "october_1_start_of_day = '2024-10-01 00:00:00'\n",
    "october_31_end_of_day = '2024-10-31 23:59:59'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_data_on_trade_datetime(data: pd.DataFrame, start_datetime_as_string: str, end_datetime_as_string: str) -> pd.DataFrame:\n",
    "    return restrict_data_to_specified_time_window(data, 'trade_datetime', start_datetime_as_string, end_datetime_as_string)\n",
    "\n",
    "\n",
    "def restrict_data_to_october_on_trade_datetime(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    return restrict_data_on_trade_datetime(data, october_1_start_of_day, october_31_end_of_day)\n",
    "\n",
    "\n",
    "def restrict_data_from_may_to_october_on_trade_datetime(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    return restrict_data_on_trade_datetime(data, may_1_start_of_day, october_31_end_of_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data from Google Cloud Storage (or locally saved files) and restrict to desired dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_DIRECTORY = f'{WORKING_DIRECTORY}/files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_file_path = f'{FILES_DIRECTORY}/old_data.pkl'\n",
    "if os.path.isfile(old_data_file_path):\n",
    "    old_data = pd.read_pickle(old_data_file_path)\n",
    "else:\n",
    "    old_data = download_data(STORAGE_CLIENT, BUCKET_NAME, MODEL_TO_CUMULATIVE_DATA_PICKLE_FILENAME[MODEL])\n",
    "    old_data.to_pickle(old_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = restrict_data_from_may_to_october_on_trade_datetime(old_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_from_sp_data_for_modeling_bucket(file_names: list) -> pd.DataFrame:\n",
    "    '''Download each file in `file_names` and concatenate the dataframes together.'''\n",
    "    return pd.concat([download_data(STORAGE_CLIENT, 'sp_data_for_modeling', file_name) for file_name in file_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_file_path = f'{FILES_DIRECTORY}/new_data.pkl'\n",
    "if os.path.isfile(new_data_file_path):\n",
    "    new_data = pd.read_pickle(new_data_file_path)\n",
    "else:\n",
    "    new_data = download_files_from_sp_data_for_modeling_bucket(['trades_2024-05-01_to_2024-05-31.pkl', \n",
    "                                                                'trades_2024-06-01_to_2024-06-30.pkl', \n",
    "                                                                'trades_2024-07-01_to_2024-07-31.pkl', \n",
    "                                                                'trades_2024-08-01_to_2024-08-31.pkl', \n",
    "                                                                'trades_2024-09-01_to_2024-09-30.pkl', \n",
    "                                                                'trades_2024-10-01_to_2024-10-31.pkl'])\n",
    "    new_data.to_pickle(new_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = restrict_data_from_may_to_october_on_trade_datetime(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_shapes(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Dataset Shapes ===')\n",
    "    print(f'Dataset 1 Shape: {df1.shape}')\n",
    "    print(f'Dataset 2 Shape: {df2.shape}')\n",
    "    num_rows_df1, num_rows_df2 = df1.shape[0], df2.shape[0]\n",
    "    num_rows_difference = num_rows_df1 - num_rows_df2\n",
    "    if num_rows_difference == 0:\n",
    "        print('Both datasets have the same number of rows')\n",
    "    elif num_rows_difference > 0:\n",
    "        print(f'Dataset 1 has {num_rows_difference} ({round(num_rows_difference / num_rows_df2 * 100, 3)}%) more rows than Dataset 2')\n",
    "    elif num_rows_difference < 0:\n",
    "        print(f'Dataset 2 has {abs(num_rows_difference)} ({round(abs(num_rows_difference) / num_rows_df1 * 100, 3)}%) more rows than Dataset 1')\n",
    "    else:\n",
    "        raise ValueError(f'{num_rows_difference} has a value that cannot be compared to 0')\n",
    "\n",
    "\n",
    "def compare_columns(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Column Comparison ===')\n",
    "    print(f'Dataset 1 Columns: {sorted(df1.columns.tolist())}')\n",
    "    print(f'Dataset 2 Columns: {sorted(df2.columns.tolist())}')\n",
    "\n",
    "    print('\\n=== Common and Unique Columns ===')\n",
    "    common_cols = set(df1.columns).intersection(set(df2.columns))\n",
    "    unique_to_df1 = set(df1.columns) - set(df2.columns)\n",
    "    unique_to_df2 = set(df2.columns) - set(df1.columns)\n",
    "    print(f'Common Columns: {sorted(common_cols)}')\n",
    "    print(f'Columns only in Dataset 1: {sorted(unique_to_df1)}')\n",
    "    print(f'Columns only in Dataset 2: {sorted(unique_to_df2)}')\n",
    "\n",
    "\n",
    "def compare_data_types(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Data Types ===')\n",
    "    print('Dataset 1 Data Types:')\n",
    "    print(df1.dtypes)\n",
    "    print('\\nDataset 2 Data Types:')\n",
    "    print(df2.dtypes)\n",
    "\n",
    "    ## below code does not work if there is a column with dtype numpy array\n",
    "    # print('\\n=== Unique Values per Column ===')\n",
    "    # print('Dataset 1 Unique Values:')\n",
    "    # print(df1.nunique())\n",
    "    # print('\\nDataset 2 Unique Values:')\n",
    "    # print(df2.nunique())\n",
    "\n",
    "\n",
    "def missing_values(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Missing Values ===')\n",
    "    print('Dataset 1 Missing Values:')\n",
    "    missing_df1 = df1.isnull().sum()\n",
    "    print(missing_df1[missing_df1 > 0])\n",
    "    \n",
    "    print('\\nDataset 2 Missing Values:')\n",
    "    missing_df2 = df2.isnull().sum()\n",
    "    print(missing_df2[missing_df2 > 0])\n",
    "\n",
    "\n",
    "def check_last_trade_in_history(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    print('\\n=== Last Trade in History ===')\n",
    "    columns_to_check = ['last_rtrs_control_number']\n",
    "    column_to_merge_on = 'rtrs_control_number'\n",
    "    columns_to_keep = [column_to_merge_on] + columns_to_check\n",
    "    assert all([((column in df1.columns) and (column in df2.columns)) for column in columns_to_keep]), f'Not all columns in {columns_to_keep} are present in both datasets'\n",
    "    df1, df2 = df1[columns_to_keep], df2[columns_to_keep]\n",
    "    suffix1, suffix2 = '_df1', '_df2'\n",
    "    merged_df = pd.merge(df1, df2, on=column_to_merge_on, suffixes=(suffix1, suffix2))\n",
    "    for column in columns_to_check:\n",
    "        col1, col2 = merged_df[column + suffix1], merged_df[column + suffix2]\n",
    "        differences = ~((col1.isna() & col2.isna()) | (col1 == col2))\n",
    "        print(f'{column}: {differences.sum()} rows have different values for the same {column_to_merge_on}')\n",
    "\n",
    "\n",
    "def statistical_summary(df1: pd.DataFrame, df2: pd.DataFrame) -> None:\n",
    "    '''`.describe(...)` has issues if there is a column with dtype numpy array.'''\n",
    "    print('\\n=== Statistical Summary ===')\n",
    "    print('Dataset 1 Summary:')\n",
    "    print(df1.describe(include='all'))\n",
    "    print('\\nDataset 2 Summary:')\n",
    "    print(df2.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_shapes(old_data, new_data)\n",
    "compare_columns(old_data, new_data)\n",
    "# compare_data_types(old_data, new_data)\n",
    "missing_values(old_data, new_data)\n",
    "check_last_trade_in_history(old_data, new_data)\n",
    "# statistical_summary(old_data, new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yield spread with similar trades model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_features_for_each_trade_in_history() -> int:\n",
    "    optional_arguments_for_process_data = get_optional_arguments_for_process_data(MODEL)\n",
    "    use_treasury_spread = optional_arguments_for_process_data.get('use_treasury_spread', False)\n",
    "    trade_history_features = get_ys_trade_history_features(use_treasury_spread)\n",
    "    return len(trade_history_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data: pd.DataFrame, \n",
    "                last_trade_date_for_training_dataset: str, \n",
    "                model_file_path: str):\n",
    "    '''Heavily inspired by `automated_trianing_auxiliary_functions::train_model(...)`. The main changes are: \n",
    "    (1) assume that we are using the yield spread with similar trades model,\n",
    "    (2) do not have an exclusions function\n",
    "    (3) do not restrict the test set to just a single day\n",
    "    '''\n",
    "    check_that_model_is_supported(MODEL)\n",
    "    encoders, fmax = fit_encoders(data, CATEGORICAL_FEATURES, MODEL)\n",
    "    test_data = data[data.trade_date > last_trade_date_for_training_dataset]    # `test_data` can only contain trades after `last_trade_date_for_training_dataset`\n",
    "    train_data = data[data.trade_date <= last_trade_date_for_training_dataset]    # `train_data` only contains trades before and including `last_trade_date_for_training_dataset`\n",
    "    training_set_info = f'Training set contains {len(train_data)} trades ranging from trade datetimes of {train_data.trade_datetime.min()} to {train_data.trade_datetime.max()}'\n",
    "    test_set_info = f'Test set contains {len(test_data)} trades ranging from trade datetimes of {test_data.trade_datetime.min()} to {test_data.trade_datetime.max()}'\n",
    "    print(training_set_info)\n",
    "    print(test_set_info)\n",
    "\n",
    "    x_train, y_train = create_input(train_data, encoders, MODEL)\n",
    "    x_test, y_test = create_input(test_data, encoders, MODEL)\n",
    "\n",
    "    keras_model = MODEL_NAME_TO_KERAS_MODEL[MODEL]\n",
    "    untrained_model = keras_model(x_train, \n",
    "                                  NUM_TRADES_IN_HISTORY_YIELD_SPREAD_MODEL, \n",
    "                                  get_num_features_for_each_trade_in_history(), \n",
    "                                  CATEGORICAL_FEATURES, \n",
    "                                  NON_CAT_FEATURES, \n",
    "                                  BINARY, \n",
    "                                  fmax)\n",
    "    trained_model, mae, history = train_and_evaluate_model(untrained_model, x_train, y_train, x_test, y_test)\n",
    "    result_df = create_summary_of_results(trained_model, test_data, x_test, y_test)\n",
    "    save_model(trained_model, None, MODEL, model_file_path, upload_to_google_cloud_bucket=False)    # setting `encoders=None` to not save the encoders file\n",
    "    return result_df, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_directory = f'{FILES_DIRECTORY}/saved_models/data_2024-05-01_2024-10-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_model(data: pd.DataFrame, last_trade_date_for_training_dataset: str, model_file_path: str) -> keras.Model:\n",
    "    '''Train the model if it does not exist in `model_file_path`. Otherwise, load the model.'''\n",
    "    if os.path.isdir(model_file_path):\n",
    "        encoders, _ = fit_encoders(data, CATEGORICAL_FEATURES, MODEL)\n",
    "        trained_model = keras.models.load_model(model_file_path)\n",
    "        test_data = data[data.trade_date > last_trade_date_for_training_dataset]\n",
    "        x_test, y_test = create_input(test_data, encoders, MODEL)\n",
    "        result_df = create_summary_of_results(trained_model, test_data, x_test, y_test)\n",
    "    else:\n",
    "        result_df, trained_model = train_model(data, last_trade_date_for_training_dataset, model_file_path)\n",
    "    return result_df, trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, model_old_data = get_trained_model(old_data, '2024-09-30', f'{saved_model_directory}/old_data_2024-11-19')    # Tuesday 2024-10-01 - Thursday 2024-10-31 is the test set\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, model_new_data = get_trained_model(new_data, '2024-09-30', f'{saved_model_directory}/new_data_2024-11-19')    # Tuesday 2024-10-01 - Thursday 2024-10-31 is the test set\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yield to price conversion\n",
    "Some trades are not able to be able to be converted from yield to price because there are certain important features missing. The server exclusions, i.e., instances in which we refuse to price a hypothetical trade, refuse to price these trades.\n",
    "\n",
    "One particular instance noticed in data from the new dataset are trades that are missing some important features needed for the conversion. The old dataset has no trades with that meet this criteria because there is a differnt labeling of the `coupon_type` feature. Below is a code snippet that returns a non-zero value for the new dataset but zero for the old dataset.\n",
    "```python\n",
    "def num_trades_meeting_exclusion_conditions(data: pd.DataFrame) -> int:\n",
    "    return ((data['coupon_type'] != 0) & \n",
    "            (data['interest_payment_frequency'] != 'Interest at maturity') & \n",
    "            data['first_coupon_date'].isnull()\n",
    "           ).sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_server_exclusions(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Applies some of the basic exclusions on the server, i.e., instances in which \n",
    "    we refuse to price a hypothetical trade.'''\n",
    "    previous_size = len(data)\n",
    "    rows_missing_important_dates = data.apply(missing_important_dates_or_dates_are_out_of_bounds, axis=1)\n",
    "    data = data[~rows_missing_important_dates]\n",
    "    rows_missing_important_features = data.apply(missing_important_features, axis=1)\n",
    "    data = data[~rows_missing_important_features]\n",
    "    current_size = len(data)\n",
    "    if previous_size != current_size: print(f'Removed {previous_size - current_size} trades for missing important dates and/or important features')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yield_spread_predictions(data: pd.DataFrame, model: keras.Model) -> np.ndarray:\n",
    "    '''Returns yield spread predictions based on calling `model` with `data`.'''\n",
    "    encoders, _ = fit_encoders(data, CATEGORICAL_FEATURES, MODEL)\n",
    "    inputs, _ = create_input(data, encoders, MODEL, ignore_label=True)\n",
    "    yield_spread_predictions = model.predict(inputs, batch_size=BATCH_SIZE).flatten()\n",
    "    return yield_spread_predictions\n",
    "\n",
    "\n",
    "def get_dollar_price_conversions(data: pd.DataFrame, yield_spread_predictions: np.ndarray) -> np.ndarray:\n",
    "    '''Returns dollar price conversions of yields resulting from adding the ficc yield curve \n",
    "    level to the predicted yield spread.'''\n",
    "    yield_predictions = yield_spread_predictions + data['ficc_ycl']\n",
    "    # assert 'ficc_ytw' not in data.columns, f'ficc_ytw is already a column in the inputted dataframe'\n",
    "    data['ficc_ytw'] = yield_predictions\n",
    "\n",
    "    converted_dollar_prices = data.apply(lambda row: compute_price(row)[0], axis=1)    # using index 0 because the first item is the dollar price and the second item is the calc date\n",
    "    data = data.drop(columns='ficc_ytw')\n",
    "    return converted_dollar_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_column_csv(csv_file_path: str) -> list:\n",
    "    data_list = []\n",
    "    with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:    # iterate through the rows in the CSV\n",
    "            data_list.append(row[0])    # assuming one column, add the first element of each row to the list\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_call_defeased_and_refund_price(data: pd.DataFrame, file_path_with_rtrs_control_numbers_to_exclude: str = None) -> pd.DataFrame:\n",
    "    '''Both `call_defeased` and `refund_price` are needed to perform yield to price conversion. However, both \n",
    "    are rarely used. `refund_price` has the value 100 in over 99% of cases (see query below). `call_defeased` \n",
    "    is set to `True` because it is also rarely used, and Jesse notes that MSRB does not use it in their price \n",
    "    conversions. This is important because further downstream, we compare our converted prices to MSRB prices.\n",
    "    \n",
    "    WITH total AS (SELECT COUNT(*) AS total_count FROM `eng-reactor-287421.jesse_tests.trade_history_same_issue_5_yr_mat_bucket_1_materialized` WHERE  is_called),\n",
    "         refund_price_100 AS (SELECT COUNT(*) AS refund_price_count FROM `eng-reactor-287421.jesse_tests.trade_history_same_issue_5_yr_mat_bucket_1_materialized` WHERE refund_price = 100 AND is_called)\n",
    "    SELECT round(SAFE_DIVIDE(refund_price_100.refund_price_count, total.total_count) * 100, 2) AS percentage_par_refund FROM total, refund_price_100;\n",
    "    '''\n",
    "    if 'refund_price' not in data.columns: data['refund_price'] = 100\n",
    "    if 'call_defeased' not in data.columns: data['call_defeased'] = True\n",
    "    print('`call_defeased` and `refund_price` have been added to the input dataframe')\n",
    "    if file_path_with_rtrs_control_numbers_to_exclude is not None:\n",
    "        rtrs_control_numbers_to_exclude = read_one_column_csv(file_path_with_rtrs_control_numbers_to_exclude)\n",
    "        rtrs_control_numbers_to_exclude = set(rtrs_control_numbers_to_exclude)\n",
    "        previous_size = len(data)\n",
    "        data = data[~data['rtrs_control_number'].isin(rtrs_control_numbers_to_exclude)]\n",
    "        current_size = len(data)\n",
    "        if previous_size != current_size: print(f'Removed {previous_size - current_size} trades for having an RTRS control number in the exclusions list')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrs_control_numbers_to_exclude_csv_file_path = f'{FILES_DIRECTORY}/rtrs_control_number_for_trades_where_refund_price_is_not_100.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yield_spread_predictions_and_dollar_price_conversions(data: pd.DataFrame, \n",
    "                                                              model: keras.Model, \n",
    "                                                              yield_spreads_file_path: str, \n",
    "                                                              dollar_prices_file_path: str) -> list[np.ndarray, np.ndarray]:\n",
    "    if os.path.isfile(yield_spreads_file_path):\n",
    "        with open(yield_spreads_file_path, 'rb') as file:\n",
    "            yield_spreads = pickle.load(file)\n",
    "    else:\n",
    "        yield_spreads = get_yield_spread_predictions(data, model)\n",
    "        with open(yield_spreads_file_path, 'wb') as file:\n",
    "            pickle.dump(yield_spreads, file)\n",
    "\n",
    "    if os.path.isfile(dollar_prices_file_path):\n",
    "        with open(dollar_prices_file_path, 'rb') as file:\n",
    "            dollar_prices = pickle.load(file)\n",
    "    else:\n",
    "        dollar_prices = get_dollar_price_conversions(data, yield_spreads)\n",
    "        with open(dollar_prices_file_path, 'wb') as file:\n",
    "            pickle.dump(dollar_prices, file)\n",
    "\n",
    "    return yield_spreads, dollar_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_with_exclusions = apply_server_exclusions(old_data)\n",
    "old_data_with_exclusions = add_call_defeased_and_refund_price(old_data_with_exclusions, rtrs_control_numbers_to_exclude_csv_file_path)\n",
    "old_data_with_exclusions_yield_spreads, old_data_with_exclusions_dollar_prices = get_yield_spread_predictions_and_dollar_price_conversions(old_data_with_exclusions, \n",
    "                                                                                                                                           model_old_data, \n",
    "                                                                                                                                           f'{FILES_DIRECTORY}/old_data_with_exclusions_yield_spreads.pkl', \n",
    "                                                                                                                                           f'{FILES_DIRECTORY}/old_data_with_exclusions_dollar_prices.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_with_exclusions = apply_server_exclusions(new_data)\n",
    "new_data_with_exclusions = add_call_defeased_and_refund_price(new_data_with_exclusions, rtrs_control_numbers_to_exclude_csv_file_path)\n",
    "new_data_with_exclusions_yield_spreads, new_data_with_exclusions_dollar_prices = get_yield_spread_predictions_and_dollar_price_conversions(new_data_with_exclusions, \n",
    "                                                                                                                                           model_new_data, \n",
    "                                                                                                                                           f'{FILES_DIRECTORY}/new_data_with_exclusions_yield_spreads.pkl', \n",
    "                                                                                                                                           f'{FILES_DIRECTORY}/new_data_with_exclusions_dollar_prices.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply exclusions on bonds close to being called or maturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dollar_price_column_name = 'ficc_dollar_price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_exclusions_and_add_dollar_prices(data: pd.DataFrame, \n",
    "                                           dollar_prices: np.ndarray, \n",
    "                                           dataset_name: str = None) -> pd.DataFrame:\n",
    "    assert converted_dollar_price_column_name not in data.columns\n",
    "    data[converted_dollar_price_column_name] = dollar_prices\n",
    "    return apply_exclusions(old_data_with_exclusions, dataset_name)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data_with_exclusions = apply_exclusions_and_add_dollar_prices(old_data_with_exclusions, old_data_with_exclusions_dollar_prices, 'old data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_with_exclusions = apply_exclusions_and_add_dollar_prices(new_data_with_exclusions, new_data_with_exclusions_dollar_prices, 'new data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot accuracy plots comparing the converted dollar price to the actual dollar price. Running the below cells requires the `matplotlib` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_plots_and_aggregate_statistics(data: pd.DataFrame) -> None:\n",
    "    # scatter plot\n",
    "    data.plot.scatter(x='dollar_price', y=converted_dollar_price_column_name, color='DarkBlue', label='Data Points')\n",
    "    ## draw the y = x line\n",
    "    min_val = min(data['dollar_price'].min(), data[converted_dollar_price_column_name].min())\n",
    "    max_val = max(data['dollar_price'].max(), data[converted_dollar_price_column_name].max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='y = x')\n",
    "    plt.show()\n",
    "\n",
    "    # price delta histogram\n",
    "    price_delta = abs(data[converted_dollar_price_column_name] - data['dollar_price'])\n",
    "    plt.hist(price_delta)\n",
    "    plt.show()\n",
    "\n",
    "    # aggregate statistics\n",
    "    print(f'Sum of errors: {np.sum(price_delta)}')\n",
    "    print(f'Mean of errors: {np.mean(price_delta)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plots_and_aggregate_statistics(old_data_with_exclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_plots_and_aggregate_statistics(new_data_with_exclusions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
